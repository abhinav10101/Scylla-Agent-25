{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28805faf-5fc8-49b7-9c1a-bba5f4a66bcc",
   "metadata": {},
   "source": [
    "# Self-RAG Pipeline with LlamaIndex, Ollama and Groq\n",
    "\n",
    "[**Paper**](https://arxiv.org/pdf/2310.11511) on Self-RAG\n",
    "\n",
    "This notebook implements a **Self-Reflective Retrieval-Augmented Generation (Self-RAG)** pipeline using [`llama-index`](https://llamaindex.ai), local embedding models via **Ollama**, and a hosted LLM from **Groq (LLaMA3 70B)**.\n",
    "\n",
    "The goal is to simulate a smart query engine that decides whether it needs to retrieve context or can directly answer a query.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa85fe5-6719-4ad4-b1c0-0910a423f3e7",
   "metadata": {},
   "source": [
    "## 1. Setup Language Models\n",
    "\n",
    "We begin by initializing:\n",
    "- The LLM using Groq's `llama3-70b-8192` model.\n",
    "- The embedding model using Ollama's local `nomic-embed-text` for generating vector representations of text.\n",
    "\n",
    "This forms the foundation for *semantic search* and question answering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "055aab5e-6144-49a1-afc6-e8b4be0a2162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8397427c-d0e6-4f51-a10b-4b8dac08a4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/itspriiyanshu/Desktop/Scylla-Agent-25/240810_Priyanshu_Ranjan/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.groq import Groq\n",
    "llm = Groq(model=\"llama3-70b-8192\")\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "embed_model = OllamaEmbedding(\n",
    "    model_name=\"nomic-embed-text:latest\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab721f20-2f4a-4c60-9bda-db9d484d00c9",
   "metadata": {},
   "source": [
    "## 2. Load, Chunk, and Embed Data\n",
    "\n",
    "We load documents from local directory using `SimpleDirectoryReader`. These documents are then embedded using the configured embedding model.\n",
    "\n",
    "These embeddings will later be stored in a vector index for retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "beaaec0c-825e-4af6-a5f7-c3e53f6e7b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78cd72e0-c94b-4626-94d7-a8c88ba60645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "parser = SentenceSplitter(chunk_size=256, chunk_overlap=16)\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e6e98d00-6ad3-46d4-8866-a9fff7b10b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\n",
    "# index.storage_context.persist(persist_dir=\"./storage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041a327b-3935-4071-a833-20b71296fe89",
   "metadata": {},
   "source": [
    "## Create Workflow\n",
    "This is the heart of our implementation.\n",
    "We define multiple Events(user-defined pydantic objects) to setup our workflow.\n",
    "\n",
    "**Workflow logic:**\n",
    "- Determine if retrieval is necessary or not.\n",
    "- If not generate response using LLM alone.\n",
    "- If retrieval is necessary, go ahead with retrieving top_3 nodes using `VectorIndexRetriever`.\n",
    "- Use LLM to determine if retrieved context is relevant to query or not.\n",
    "- If no context deemed relevant proceed to generate response using LLM alone.\n",
    "- If relevant context found, augment it with query in a custom prompt and use it to generate response via LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "09955da3-a8e4-44e5-94ac-db932f73be5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "srag.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"600\"\n",
       "            src=\"srag.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x78bd0cfe4170>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.utils.workflow import (\n",
    "    draw_all_possible_flows,\n",
    "    draw_most_recent_execution,\n",
    ")\n",
    "from IPython.display import IFrame\n",
    "\n",
    "\n",
    "# Draw all\n",
    "draw_all_possible_flows(sRAG, filename=\"srag.html\")\n",
    "IFrame(\"srag.html\", width=800, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3dc825d6-9678-4808-8a23-fe116e9351be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import (\n",
    "    StartEvent,\n",
    "    StopEvent,\n",
    "    Workflow,\n",
    "    step,\n",
    "    Event,\n",
    ")\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from typing import List\n",
    "from llama_index.core.schema import TextNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81f36ef5-d287-4304-9195-c6c561e2de7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoRetrieval(Event):\n",
    "    query: str\n",
    "\n",
    "class RetrieveEvent(Event):\n",
    "    query: str\n",
    "\n",
    "class RelevanceEval(Event):\n",
    "    query: str\n",
    "    retrieved_nodes: List[TextNode]\n",
    "\n",
    "class WithRetrieval(Event):\n",
    "    query: str\n",
    "    relevant_context: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "99472243-04d4-4561-8414-9f990a65d376",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sRAG(Workflow):\n",
    "    @step\n",
    "    async def decide_retrieval(self, ev: StartEvent) -> RetrieveEvent | NoRetrieval:\n",
    "        query = ev.query\n",
    "\n",
    "        prompt = f\"Given the query: '{query}', determine if retrieval is necessary. Output only 'Yes' or 'No'.\"\n",
    "        response = await llm.acomplete(prompt)\n",
    "        if str(response)==\"Yes\":\n",
    "            return RetrieveEvent(query=query)\n",
    "        else:\n",
    "            return NoRetrieval(query=query)\n",
    "    @step\n",
    "    async def retrieve(self, ev: RetrieveEvent) -> RelevanceEval:\n",
    "        query = ev.query\n",
    "        # storage_context = StorageContext.from_defaults(persist_dir=\"./storage\", embed_model=embed_model)\n",
    "        # index = load_index_from_storage(storage_context)\n",
    "        retriever = VectorIndexRetriever(index=index, similarity_top_k=3)\n",
    "        retrieved_nodes = retriever.retrieve(query)\n",
    "        text_nodes = [n.node for n in retrieved_nodes] #try sending with scores as well and make llm leverage them evaluate relevange\n",
    "        return RelevanceEval(query=query, retrieved_nodes=text_nodes)\n",
    "    @step\n",
    "    async def eval_relevance(self, ev: RelevanceEval) -> WithRetrieval | NoRetrieval:\n",
    "        retrieved_nodes = ev.retrieved_nodes\n",
    "        query = ev.query\n",
    "        relevant_context = []\n",
    "        for node in retrieved_nodes:\n",
    "            context = node.get_content()\n",
    "            prompt = f\"Given the query: '{query}' and the context: '{context}', determine if the context is relevant. Output only 'Relevant' or 'Irrelevant'.\"\n",
    "            response = await llm.acomplete(prompt)\n",
    "            if str(response)==\"Relevant\":\n",
    "                relevant_context.append(context)\n",
    "        if not relevant_context:\n",
    "            return NoRetrieval(query=query)\n",
    "        else:\n",
    "            return WithRetrieval(query=query, relevant_context=relevant_context)\n",
    "    @step\n",
    "    async def generate_with_context(self, ev: WithRetrieval) -> StopEvent:\n",
    "        query= ev.query\n",
    "        relevant_context = ev.relevant_context\n",
    "        context = \"\\n\".join(f\"- {doc}\" for doc in relevant_context)\n",
    "        # this has a flaw, joining contexts may lead to redundant data and possible chunk overlaps\n",
    "        # instead of joining contexts, we can try generating response over individual contexts \n",
    "        prompt = f\"Given the query '{query}' and the context '{context}', generate a response.\"\n",
    "        response = await llm.acomplete(prompt)\n",
    "        # print(str(response))\n",
    "        return StopEvent(result = str(response))\n",
    "    @step\n",
    "    async def generate_without_context(self, ev: NoRetrieval) -> StopEvent:\n",
    "        query= ev.query\n",
    "        prompt = f\"Given the query '{query}', generate a response.\"\n",
    "        response = await llm.acomplete(prompt)\n",
    "        # print(str(response))\n",
    "        return StopEvent(result = str(response))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dcf42f-ba8e-4d4a-9cac-b39a1b2b2ed4",
   "metadata": {},
   "source": [
    "# Running the Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d589cd-50fb-48af-aa55-68afc14c5cd7",
   "metadata": {},
   "source": [
    "### Example where **No Retrieval** needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "046689de-e0bc-41e7-88db-cc0d2b11d298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step decide_retrieval\n",
      "Step decide_retrieval produced event NoRetrieval\n",
      "Running step generate_without_context\n",
      "Step generate_without_context produced event StopEvent\n",
      "Here's a joke for you:\n",
      "\n",
      "Why couldn't the bicycle stand up by itself?\n",
      "\n",
      "Because it was two-tired!\n",
      "\n",
      "Hope that made you laugh!\n"
     ]
    }
   ],
   "source": [
    "w = sRAG(timeout=120, verbose = True)\n",
    "result = await w.run(query=\"Write a joke\")\n",
    "print(str(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae95b1f5-7833-4648-8056-3da02dba7633",
   "metadata": {},
   "source": [
    "### Example where **Retrieval** needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dee491d0-35c0-42cc-bbe7-2ef29251aa48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step decide_retrieval\n",
      "Step decide_retrieval produced event RetrieveEvent\n",
      "Running step retrieve\n",
      "Step retrieve produced event RelevanceEval\n",
      "Running step eval_relevance\n",
      "Step eval_relevance produced event WithRetrieval\n",
      "Running step generate_with_context\n",
      "Step generate_with_context produced event StopEvent\n",
      "Here is a response to the query \"Review for EE200A course\":\n",
      "\n",
      "The EE200A course, also known as Signals, Systems & Networks, is a fundamental course in the Electrical Engineering (EE) curriculum. According to the review, this course is the first true EE course that students will encounter on campus, and it deals with signals and their representations, systems, Fourier representations, and networks.\n",
      "\n",
      "The instructor for this course is reportedly one of the best on campus, which is a positive aspect. However, the course has some drawbacks. Notes are not provided, and students have to write notes in class, which can be challenging. Additionally, the assignments are lengthy, and the questions asked in the tests are often different from what is expected.\n",
      "\n",
      "Past papers are a great way to practice, but solutions are hard to find, making it difficult for students to know if their answers are correct or not. Overall, the difficulty level of this course is rated as 4 out of 5.\n",
      "\n",
      "In summary, EE200A is a crucial course in the EE curriculum, and while it has some challenges, it is essential to have a good instructor to guide students through the course.\n"
     ]
    }
   ],
   "source": [
    "w = sRAG(timeout=120, verbose = True)\n",
    "result = await w.run(query=\"Review for EE200A course\")\n",
    "print(str(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
